{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAWXQYiMUjM3",
        "outputId": "27ee7122-4500-4d3a-f1c5-65c4dc27eb2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from nltk.tokenize import word_tokenize  # or any specific tokenizer you prefer from nltk\n",
        "import nltk\n",
        "nltk.download('punkt')  # If using NLTK's default tokenizer\n",
        "\n",
        "# Optional based on your needs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "# File path to your JSON dataset\n",
        "file_path = '/content/arcd-test.json'\n",
        "\n",
        "# Load the JSON file\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    full_data = json.load(file)\n",
        "\n",
        "# Function to tokenize text by splitting on spaces\n",
        "def basic_tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "# Tokenizing the dataset (context and questions only, answers remain untokenized)\n",
        "tokenized_full_data = []\n",
        "for entry in full_data['data']:\n",
        "    for paragraph in entry['paragraphs']:\n",
        "        context = paragraph['context']\n",
        "        tokenized_context = basic_tokenize(context)\n",
        "        tokenized_qas = []\n",
        "        for qa in paragraph['qas']:\n",
        "            question = qa['question']\n",
        "            tokenized_question = basic_tokenize(question)\n",
        "            answers = qa['answers']  # Keeping answers untokenized\n",
        "            tokenized_qas.append({'question': tokenized_question, 'id': qa['id'], 'answers': answers})\n",
        "        tokenized_full_data.append({'context': tokenized_context, 'qas': tokenized_qas})\n",
        "\n",
        "# Initialize the BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"salti/AraElectra-base-finetuned-ARCD\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"salti/AraElectra-base-finetuned-ARCD\")\n",
        "\n",
        "# Function to encode the data for BERT\n",
        "def encode_data(data, max_length=512):\n",
        "    encoded_data = []\n",
        "    for item in data:\n",
        "        context = ' '.join(item['context'])\n",
        "        for qa in item['qas']:\n",
        "            question = ' '.join(qa['question'])\n",
        "            answer_text = qa['answers'][0]['text']\n",
        "            start_position = qa['answers'][0]['answer_start']\n",
        "\n",
        "            # Encode and pad for Electra with truncation\n",
        "            inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, max_length=max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
        "            answer_start_position = context.find(answer_text)\n",
        "            answer_end_position = answer_start_position + len(answer_text)\n",
        "\n",
        "            if answer_start_position < max_length:  # Making sure the answer is not truncated\n",
        "                encoded_data.append({'input_ids': inputs['input_ids'].squeeze(0),\n",
        "                                    'attention_mask': inputs['attention_mask'].squeeze(0),\n",
        "                                    'start_positions': torch.tensor(answer_start_position),\n",
        "                                    'end_positions': torch.tensor(answer_end_position)})\n",
        "    return encoded_data\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "def custom_train_test_split(data, test_size=0.2):\n",
        "    # Calculate the number of samples for the test set\n",
        "    n_total = len(data)\n",
        "    n_test = int(n_total * test_size)\n",
        "\n",
        "    # Randomly shuffle the data indices\n",
        "    indices = list(range(n_total))\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    # Split indices into training and test sets\n",
        "    test_indices = indices[:n_test]\n",
        "    train_indices = indices[n_test:]\n",
        "\n",
        "    # Create training and test datasets\n",
        "    train_data = [data[i] for i in train_indices]\n",
        "    test_data = [data[i] for i in test_indices]\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "# Use the custom function to split the data\n",
        "#train_data, val_data = custom_train_test_split(encoded_data, test_size=0.2)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "#train_data, val_data = train_test_split(encoded_data, test_size=0.2)\n",
        "\n",
        "# The `train_data` and `val_data` are now ready for training and validation\n"
      ],
      "metadata": {
        "id": "tbNUYQsEOtXZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import random\n",
        "\n",
        "# Load the JSON file\n",
        "file_path = '/content/arcd-test.json'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    full_data = json.load(file)\n",
        "\n",
        "# Initialize the Electra tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"salti/AraElectra-base-finetuned-ARCD\")\n",
        "\n",
        "# Function to encode the data for Electra\n",
        "def encode_data(data, max_length=512):\n",
        "    encoded_data = []\n",
        "    for entry in data['data']:\n",
        "        for paragraph in entry['paragraphs']:\n",
        "            context = paragraph['context']\n",
        "            for qa in paragraph['qas']:\n",
        "                question = qa['question']\n",
        "                answer_text = qa['answers'][0]['text']\n",
        "                answer_start = qa['answers'][0]['answer_start']\n",
        "                answer_end = answer_start + len(answer_text)\n",
        "\n",
        "                # Encode and pad for Electra with truncation\n",
        "                inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, max_length=max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
        "                answer_start_position = context.find(answer_text)\n",
        "                answer_end_position = answer_start_position + len(answer_text)\n",
        "\n",
        "                # Ensure the answer isn't truncated\n",
        "                if answer_start_position < max_length and answer_end_position < max_length:\n",
        "                    encoded_data.append({'input_ids': inputs['input_ids'].squeeze(0),\n",
        "                                         'attention_mask': inputs['attention_mask'].squeeze(0),\n",
        "                                         'start_positions': torch.tensor(answer_start_position),\n",
        "                                         'end_positions': torch.tensor(answer_end_position)})\n",
        "    return encoded_data\n",
        "\n",
        "# Encode the data\n",
        "encoded_data = encode_data(full_data)\n",
        "\n",
        "# Custom train/test split function\n",
        "def custom_train_test_split(data, test_size=0.2):\n",
        "    n_total = len(data)\n",
        "    n_test = int(n_total * test_size)\n",
        "    indices = list(range(n_total))\n",
        "    random.shuffle(indices)\n",
        "    test_indices = indices[:n_test]\n",
        "    train_indices = indices[n_test:]\n",
        "    train_data = [data[i] for i in train_indices]\n",
        "    test_data = [data[i] for i in test_indices]\n",
        "    return train_data, test_data\n",
        "\n",
        "# Splitting the data\n",
        "train_data, val_data = custom_train_test_split(encoded_data, test_size=0.2)\n",
        "\n",
        "# Dataset class\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.encodings[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings)\n",
        "\n",
        "# collate_fn function\n",
        "def collate_fn(batch):\n",
        "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_mask = pad_sequence([item['attention_mask'] for item in batch], batch_first=True, padding_value=0)\n",
        "    start_positions = torch.stack([item['start_positions'] for item in batch])\n",
        "    end_positions = torch.stack([item['end_positions'] for item in batch])\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'start_positions': start_positions, 'end_positions': end_positions}\n",
        "\n",
        "# Datasets and DataLoaders\n",
        "train_dataset = QADataset(train_data)\n",
        "val_dataset = QADataset(val_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "train_loader, val_loader\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFelT_l2OPjP",
        "outputId": "13fda8f3-449f-4e8e-cfba-c7493ad7515f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x79c73b0a4be0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x79c73b0a7cd0>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "# Assuming model is already loaded\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(40):  # Adjust the number of epochs\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    for batch in val_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        # You can add your evaluation metric here\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained('your_model_directory')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yKFbEKuOZVc",
        "outputId": "913187e4-c551-4e66-ccca-30e41887d6a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "    gold_toks = set(a_gold.split())\n",
        "    pred_toks = set(a_pred.split())\n",
        "    common = gold_toks & pred_toks\n",
        "    if len(common) == 0: return 0\n",
        "    precision = len(common) / len(pred_toks)\n",
        "    recall = len(common) / len(gold_toks)\n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "def compute_em(a_gold, a_pred):\n",
        "    return int(a_gold == a_pred)\n",
        "def compute_sm(a_gold, a_pred, threshold=0.7):\n",
        "    gold_toks = set(a_gold.split())\n",
        "    pred_toks = set(a_pred.split())\n",
        "    if not gold_toks or not pred_toks:\n",
        "        return 0\n",
        "    common = gold_toks & pred_toks\n",
        "    similarity = len(common) / max(len(gold_toks), len(pred_toks))\n",
        "    return int(similarity >= threshold)\n",
        "\n",
        "\n",
        "# Trackers for metrics\n",
        "f1_scores = []\n",
        "em_scores = []\n",
        "sm_scores = []\n",
        "\n",
        "def get_true_answer(encoded_data, idx):\n",
        "    # Assuming the true answer is stored in the encoded data\n",
        "    answer_start = encoded_data[idx]['start_positions'].item()\n",
        "    answer_end = encoded_data[idx]['end_positions'].item() + 1\n",
        "    true_answer = tokenizer.decode(encoded_data[idx]['input_ids'][answer_start:answer_end])\n",
        "    return true_answer\n",
        "\n",
        "# In your validation loop\n",
        "for batch_idx, batch in enumerate(val_loader):\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "    for i in range(start_logits.shape[0]):\n",
        "        start_pred = torch.argmax(start_logits[i])\n",
        "        end_pred = torch.argmax(end_logits[i]) + 1\n",
        "        predicted_answer = tokenizer.decode(batch['input_ids'][i][start_pred:end_pred])\n",
        "\n",
        "        # Retrieve the true answer\n",
        "        global_idx = batch_idx * val_loader.batch_size + i\n",
        "        true_answer = get_true_answer(val_data, global_idx)\n",
        "\n",
        "        f1_scores.append(compute_f1(true_answer, predicted_answer))\n",
        "        em_scores.append(compute_em(true_answer, predicted_answer))\n",
        "        sm_scores.append(compute_sm(true_answer, predicted_answer))\n",
        "\n",
        "# Calculate average scores\n",
        "avg_f1 = sum(f1_scores) / len(f1_scores)\n",
        "avg_em = sum(em_scores) / len(em_scores)\n",
        "avg_sm = sum(sm_scores) / len(sm_scores)\n",
        "print(f\"Average F1 Score: {avg_f1}\")\n",
        "print(f\"Exact Match Score: {avg_em}\")\n",
        "print(f\"Sentence Match Score: {avg_sm}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvIc9Zd6PSJy",
        "outputId": "6af68fd8-2d55-4d4f-ce35-8ad5cb20fe24"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average F1 Score: 0.2557948678248253\n",
            "Exact Match Score: 0.0\n",
            "Sentence Match Score: 0.1951219512195122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_zuKV9I2YZMy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}